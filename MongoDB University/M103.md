# MongoDB University - M103 - Basic Cluster Administration

In this course I learnt about Basic Cluster Administration.

## Launch mongod in the shell

```bash
mongod
```

## Connect to Mongo shell

```bash
mongo
```

## Create new collection

```bash
db.createCollection("employees")
```

## Try to connect back to Mongo shell without specifying a port

```bash
use admin
db.shutdownServer()
exit
```

## Launch mongod using default configs

```bash
mongod
```

## Launch mongod with specified --dbpath and --logpath:

```bash
mongod --dbpath /data/db --logpath /data/log/mongod.log
mongod --dbpath /data/db --logpath /data/log/mongod.log --fork
```

## Launch mongod with many configuration options:

```bash
mongod --dbpath /data/db --logpath /data/log/mongod.log --fork --replSet "M103" --keyFile /data/keyfile --bind_ip "127.0.0.1,192.168.103.100" --tlsMode requireTLS --tlsCAFile "/etc/tls/TLSCA.pem" --tlsCertificateKeyFile "/etc/tls/tls.pem"
```

## Example configuration file, with the same configuration options as above:

```bash
storage:
  dbPath: "/data/db"
systemLog:
  path: "/data/log/mongod.log"
  destination: "file"
replication:
  replSetName: M103
net:
  bindIp : "127.0.0.1,192.168.103.100"
tls:
  mode: "requireTLS"
  certificateKeyFile: "/etc/tls/tls.pem"
  CAFile: "/etc/tls/TLSCA.pem"
security:
  keyFile: "/data/keyfile"
processManagement:
  fork: true
```

## Basic Commands

## User management

````bash
db.createUser()

```bash
db.createUser({
  "user": "3rfan",
  "pwd": passwordPrompt(),
  "roles": [
    { "role": "clusterAdmin", "db": "admin" },
    { "role": "readAnyDatabase", "db": "admin" }, "readWrite"
  ]
}, { "w": "majority", "wtimeout": 5000 })
````

/_ - _ - _ - _ - _ - _/

```bash
db.dropUser()
```

## Collection

````bash
db.<collection>.renameCollection()
```bash
db.<collection>.createIndex()
```bash
db.<collection>.drop()
````

## DB management

````bash
db.dropDatabase()
```bash
db.createCollection()
````

## DB status

```bash
db.serverStatus()
```

## Creating index with DB command (use shell helper instead!)

```bash
db.runCommand({
   "createIndexes":"<collection_name>",
       "indexes":[
          {
             "key":{ "product": 1 },
             "name": "name_index"
          }
       ]
    }
 )
```

## Creating index with Shell helper

```bash
db.<collection>.createIndex(
  { "product": 1 },
  { "name": "name_index" }
)
```

## Introspect a Shell helper

```bash
db.<collection>.createIndex()
```

## Get logging components

```bash
mongo admin --host 192.168.103.100:27000 -u m103-admin -p m103-pass --eval '
  db.getLogComponents()'
```

## Change logging level

```bash
mongo admin --host 192.168.103.100:27000 -u m103-admin -p m103-pass --eval '
  db.setLogLevel(0, "index")'

```

## View logs through shell

```bash
db.adminCommand({ "getLog": "global" })
```

## View logs

```bash
tail -f /data/db/mongod.log
```

## Update doc

```bash
mongo admin --host 192.168.103.100:27000 -u m103-admin -p m103-pass --eval '
  db.products.update( { "sku" : 6902667 }, { $set : { "salePrice" : 39.99} } )'
```

## Look for instructions in log file

```bash
grep -i 'update' /data/db/mongod.log
```

## Profiling the DB

## Get profiling level

```bash
mongo newDB --host 192.168.103.100:27000 -u m103-admin -p m103-pass --authenticationDatabase admin --eval '
  db.getProfilingLevel()'
```

## Set proviling level

```bash
mongo newDB --host 192.168.103.100:27000 -u m103-admin -p m103-pass --authenticationDatabase admin --eval '
  db.setProfilingLevel(1)'
```

## Show collections

```bash
mongo newDB --host 192.168.103.100:27000 -u m103-admin -p m103-pass --authenticationDatabase admin --eval '
  db.getCollectionNames()'
```

## Set slowms to 0

```bash
mongo newDB --host 192.168.103.100:27000 -u m103-admin -p m103-pass --authenticationDatabase admin --eval '
  db.setProfilingLevel( 1, { slowms: 0 } )'
```

## Insert one document into a new collection:

```bash
mongo newDB --host 192.168.103.100:27000 -u m103-admin -p m103-pass --authenticationDatabase admin --eval '
  db.new_collection.insert( { "a": 1 } )'
```

## Get profiling data from system.profile:

```bash
mongo newDB --host 192.168.103.100:27000 -u m103-admin -p m103-pass --authenticationDatabase admin --eval '
  db.system.profile.find().pretty()'

```

## Print config file

```bash
cat /opt/homebrew/etc/mongod.conf

```

## Launch standalone mongod

```bash
mongod -f /opt/homebrew/etc/mongod.conf

```

## Connect to mongod

```bash
mongo --host 127.0.0.1:27017

```

## Create new user with the root role (also, named root):

```bash
use admin
db.createUser({
  user: "root",
  pwd: "root123",
  roles : [ "root" ]
})

```

## Connect to mongod and authenticate as root:

```bash
mongo --username root --password root123 --authenticationDatabase admin

```

## Run DB stats

```bash
db.stats()

```

## Shutdown server

```bash
use admin
db.shutdownServer()

```

## Create security officer

```bash
db.createUser(
  { user: "security_officer",
    pwd: "h3ll0th3r3",
    roles: [ { db: "admin", role: "userAdmin" } ]
  }
)

```

## Create DB admin

```bash
db.createUser(
  { user: "dba",
    pwd: "c1lynd3rs",
    roles: [ { db: "admin", role: "dbAdmin" } ]
  }
)

```

## Grant role to user

```bash
db.grantRolesToUser( "dba",  [ { db: "playground", role: "dbOwner"  } ] )

```

## Show role privileges

```bash
db.runCommand( { rolesInfo: { role: "dbOwner", db: "playground" }, showPrivileges: true} )

```

## Server Tools

## List mongodb binaries

```bash
find /usr/bin/ -name "mongo*"

```

## Create new dbpath and launch mongod:

```bash
mkdir -p ~/first_mongod
mongod --port 30000 --dbpath ~/first_mongod --logpath ~/first_mongod/mongodb.log --fork

```

## Use mongostat to get stats on a running mongod process:

```bash
mongostat --help
mongostat --port 30000

```

## Use mongodump to get a BSON dump of a MongoDB collection:

```bash
mongodump --help
mongodump --port 30000 --db applicationData --collection products
ls dump/applicationData/
cat dump/applicationData/products.metadata.json

```

## Use mongorestore to restore a MongoDB collection from a BSON dump:

```bash
mongorestore --drop --port 30000 dump/

```

## Use mongoexport to export a MongoDB collection to JSON or CSV (or stdout!):

```bash
mongoexport --help
mongoexport --port 30000 --db applicationData --collection products
mongoexport --port 30000 --db applicationData --collection products -o products.json

```

## Use mongoimport to create a MongoDB collection from a JSON or CSV file:

```bash
mongoimport --port 30000 products.json

```

## Setting up Replica Set

## The configuration file for the first node (node1.conf):

```bash
storage:
  dbPath: /var/mongodb/db/node1
net:
  bindIp: 192.168.103.100,localhost
  port: 27011
security:
  authorization: enabled
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node1/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

```

## Creating the keyfile and setting permissions on it:

```bash
sudo mkdir -p /var/mongodb/pki/
sudo chown vagrant:vagrant /var/mongodb/pki/
openssl rand -base64 741 > /var/mongodb/pki/m103-keyfile
chmod 400 /var/mongodb/pki/m103-keyfile

```

## Creating the dbpath for node1:

```bash
mkdir -p /var/mongodb/db/node1

```

## Starting a mongod with node1.conf:

```bash
mongod -f node1.conf

```

## Copying node1.conf to node2.conf and node3.conf:

```bash
cp node1.conf node2.conf
cp node2.conf node3.conf

```

## Editing node2.conf using vi:

```bash
vi node2.conf
:wq

```

## node2.conf, after changing the dbpath, port, and logpath:

```bash
storage:
  dbPath: /var/mongodb/db/node2
net:
  bindIp: 192.168.103.100,localhost
  port: 27012
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node2/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

```

## node3.conf, after changing the dbpath, port, and logpath:

```bash
storage:
  dbPath: /var/mongodb/db/node3
net:
  bindIp: 192.168.103.100,localhost
  port: 27013
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node3/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

```

## Creating the data directories for node2 and node3:

```bash
mkdir /var/mongodb/db/{node2,node3}

```

## Starting mongod processes with node2.conf and node3.conf:

```bash
mongod -f node2.conf
mongod -f node3.conf

```

## Connecting to node1:

```bash
mongo --port 27011

```

## Initiating the replica set:

```bash
rs.initiate()

```

## Creating user

```bash
use admin
db.createUser({
  user: "m103-admin",
  pwd: "m103-pass",
  roles: [
    {role: "root", db: "admin"}
  ]
})

```

## Exiting out of the Mongo shell and connecting to the entire replica set:

```bash
exit
mongo --host "m103-example/192.168.103.100:27011" -u "m103-admin"
-p "m103-pass" --authenticationDatabase "admin"

```

## Getting replica set status:

```bash
rs.status()

```

## Adding other members to replica set:

```bash
rs.add("m103:27012")
rs.add("m103:27013")

```

## Getting an overview of the replica set topology:

```bash
rs.isMaster()

```

## Stepping down the current primary:

```bash
rs.stepDown()

```

## Checking replica set overview after election:

```bash
rs.isMaster()

```

## Replication commands

```bash
rs.status()
rs.isMaster()
db.serverStatus()['repl']
rs.printReplicationInfo()

```

## Make a data directory and launch a mongod process for a standalone node:

```bash
mkdir allbymyselfdb
mongod --dbpath allbymyselfdb

```

## Display all databases (by default, only admin and local):

```bash
mongo
show dbs

```

## Display collections from the local database (this displays more collections from a replica set than from a standalone node):

```bash
use local
show collections

```

## Query the oplog after connected to a replica set:

```bash
use local
db.oplog.rs.find()

```

## Store oplog stats as a variable called stats:

```bash
var stats = db.oplog.rs.stats()

```

## Verify that this collection is capped (it will grow to a pre-configured size before it starts to overwrite the oldest entries with newer ones):

```bash
stats.capped

```

## Get current size of the oplog:

```bash
stats.size

```

## Get size limit of the oplog:

```bash
stats.maxSize

```

## Get current oplog data (including first and last event times, and configured oplog size):

```bash
rs.printReplicationInfo()

```

## Local DB

````
## Create new namespace m103.messages:
```bash
use m103
db.createCollection('messages')

````

## Query the oplog, filtering out the heartbeats ("periodic noop") and only returning the latest entry:

```bash
use local
db.oplog.rs.find( { "o.msg": { $ne: "periodic noop" } } ).sort( { $natural: -1 } ).limit(1).pretty()

```

## Insert 100 different documents:

```bash
use m103
for ( i=0; i< 100; i++) { db.messages.insert( { 'msg': 'not yet', _id: i } ) }
db.messages.count()

```

## Query the oplog to find all operations related to m103.messages:

```bash
use local
db.oplog.rs.find({"ns": "m103.messages"}).sort({$natural: -1})

```

## Illustrate that one update statement may generate many entries in the oplog:

```bash
use m103
db.messages.updateMany( {}, { $set: { author: 'norberto' } } )
use local
db.oplog.rs.find( { "ns": "m103.messages" } ).sort( { $natural: -1 } )

```

## node4.conf

```bash
storage:
  dbPath: /var/mongodb/db/node4
net:
  bindIp: 192.168.103.100,localhost
  port: 27014
systemLog:
  destination: file
  path: /var/mongodb/db/node4/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

```

## arbiter.conf:

```bash
storage:
  dbPath: /var/mongodb/db/arbiter
net:
  bindIp: 192.168.103.100,localhost
  port: 28000
systemLog:
  destination: file
  path: /var/mongodb/db/arbiter/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

```

## Starting up mongod processes for our fourth node and arbiter:

```bash
mongod -f node4.conf
mongod -f arbiter.conf

```

## From the Mongo shell of the replica set, adding the new secondary and the new arbiter:

```bash
rs.add("m103:27014")
rs.addArb("m103:28000")

```

## Checking replica set makeup after adding two new nodes:

```bash
rs.isMaster()

```

## Removing the arbiter from our replica set:

```bash
rs.remove("m103:28000")

```

## Assigning the current configuration to a shell variable we can edit, in order to reconfigure the replica set:

```bash
cfg = rs.conf()

```

## Editing our new variable cfg to change topology - specifically, by modifying cfg.members:

```bash
cfg.members[3].votes = 0
cfg.members[3].hidden = true
cfg.members[3].priority = 0

```

## Updating our replica set to use the new configuration cfg:

```bash
rs.reconfig(cfg)

```

## Reads and Writes on a Replica Set

## Connecting to the replica set:

```bash
mongo --host "m103-example/m103:27011" -u "m103-admin" -p
"m103-pass" --authenticationDatabase "admin"

```

## Checking replica set topology:

```bash
rs.isMaster()

```

## Inserting one document into a new collection:

```bash
use newDB
db.new_collection.insert( { "student": "Matt Javaly", "grade": "A+" } )

```

## Connecting directly to a secondary node (this node may not be a secondary in your replica set!):

```bash
mongo --host "m103:27012" -u "m103-admin" -p "m103-pass"
--authenticationDatabase "admin"

```

## Attempting to execute a read command on a secondary node (this should fail):

```bash
show dbs

```

## Enabling read commands on a secondary node:

```bash
rs.slaveOk()

```

## Reading from a secondary node:

```bash
use newDB
db.new_collection.find()

```

## Attempting to write data directly to a secondary node (this should fail, because we cannot write data directly to a secondary):

```bash
db.new_collection.insert( { "student": "Norberto Leite", "grade": "B+" } )

```

## Shutting down the server (on both secondary nodes)

```bash
use admin
db.shutdownServer()

```

## Connecting directly to the last healthy node in our set:

```bash
mongo --host "m103:27011" -u "m103-admin" -p "m103-pass"
--authenticationDatabase "admin"

```

## Verifying that the last node stepped down to become a secondary when a majority of nodes in the set were not available:

```bash
rs.isMaster()

```

## Failover and Elections

## Storing replica set configuration as a variable cfg:

```bash
cfg = rs.conf()

```

## Setting the priority of a node to 0, so it cannot become primary (making the node "passive"):

```bash
cfg.members[2].priority = 0

```

## Updating our replica set to use the new configuration cfg:

```bash
rs.reconfig(cfg)

```

## Checking the new topology of our set:

```bash
rs.isMaster()

```

## Forcing an election in this replica set (although in this case, we rigged the election so only one node could become primary):

```bash
rs.stepDown()

```

## Checking the topology of our set after the election:

```bash
rs.isMaster()

```

## Setting up a sharded Cluster

## Configuration file for first config server csrs_1.conf:

```bash
sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26001
systemLog:
  destination: file
  path: /var/mongodb/db/csrs1.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs1

```

## csrs_2.conf:

```bash
sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26002
systemLog:
  destination: file
  path: /var/mongodb/db/csrs2.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs2

```

## csrs_3.conf:

```bash

sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26003
systemLog:
  destination: file
  path: /var/mongodb/db/csrs3.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs3

```

## Starting the three config servers:

```bash
mongod -f csrs_1.conf
mongod -f csrs_2.conf
mongod -f csrs_3.conf

```

## Connect to one of the config servers:

```bash
mongo --port 26001

```

## Initiating the CSRS:

```bash
rs.initiate()

```

## Creating super user on CSRS:

```bash
use admin
db.createUser({
  user: "m103-admin",
  pwd: "m103-pass",
  roles: [
    {role: "root", db: "admin"}
  ]
})

```

## Authenticating as the super user:

```bash
db.auth("m103-admin", "m103-pass")

```

## Add the second and third node to the CSRS:

```bash
rs.add("192.168.103.100:26002")
rs.add("192.168.103.100:26003")

```

## Mongos config (mongos.conf):

```bash
sharding:
  configDB: m103-csrs/192.168.103.100:26001,192.168.103.100:26002,192.168.103.100:26003
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26000
systemLog:
  destination: file
  path: /var/mongodb/db/mongos.log
  logAppend: true
processManagement:
  fork: true

```

## Start the mongos server:

```bash
mongos -f mongos.conf

```

## Connect to mongos:

```bash
vagrant@m103:~$ mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin

```

## Check sharding status:

```bash
MongoDB Enterprise mongos> sh.status()

```

## Updated configuration for node1.conf:

```bash
sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node1
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27011
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node1/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-repl

```

## Updated configuration for node2.conf:

```bash
sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node2
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27012
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node2/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-repl

```

## Updated configuration for node3.conf:

```bash
sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node3
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27013
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node3/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-repl

```

## Connecting directly to secondary node (note that if an election has taken place in your replica set, the specified node may have become primary):

```bash
mongo --port 27012 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

```

## Shutting down node:

```bash
use admin
db.shutdownServer()

```

## Restarting node with new configuration:

```bash
mongod -f node2.conf

```

## Stepping down current primary:

```bash
rs.stepDown()

```

## Adding new shard to cluster from mongos:

```bash
sh.addShard("m103-repl/192.168.103.100:27012")

```

## Config DB

## Switch to config DB:

````bash
use config

## Query config.databases:
```bash
db.databases.find().pretty()

````

## Query config.collections:

```bash
db.collections.find().pretty()

```

## Query config.shards:

```bash
db.shards.find().pretty()

```

## Query config.chunks:

```bash
db.chunks.find().pretty()

```

## Query config.mongos:

```bash
db.mongos.find().pretty()

```

## Shard Keys

## Show collections in m103 database:

```bash
use m103
show collections
```

## Enable sharding on the m103 database:

```bash
sh.enableSharding("m103")

```

## Find one document from the products collection, to help us choose a shard key:

```bash
db.products.findOne()

```

## Create an index on sku:

```bash
db.products.createIndex( { "sku": 1 } )

```

## Shard the products collection on sku:

```bash
sh.shardCollection( "m103.products", { "sku": 1 } )

```

## Checking the status of the sharded cluster:

```bash
sh.status()

```

## Chunks

## Show collections in config database:

```bash
use config
show collections

```

## Find one document from the chunks collection:

```bash
db.chunks.findOne()

```

## Change the chunk size:

```bash
use config
db.settings.save({_id: "chunksize", value: 2})

```

## Check the status of the sharded cluster:

```bash
sh.status()

```

## Import the new products.part2 dataset into MongoDB:

```bash
mongoimport /dataset/products.part2.json --port 26000 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin" --db m103 --collection products

```

## Balancing

## Start the balancer:

```bash
sh.startBalancer(timeout, interval)

```

## Stop the balancer:

```bash
sh.stopBalancer(timeout, interval)

```

## Enable/disable the balancer:

```bash
sh.setBalancerState(boolean)

```

## Targeted Queries vs Scatter Gather: Part 1

## A find() query where the predicate matches the shard key, and the query can be targeted:

```bash
db.products.find( { "sku": 20009151 } )

```

## A find() query where the predicate does not match the shard key, and the query is scatter gather:

```bash
db.products.find( { "type": "movie" } )

```

## Example of a compound shard key:

```bash
{ "sku": 1, "type": 1, "name": 1 }

```

## Examples of queries that would be targeted with the compound shard key:

```bash
db.products.find( { "sku": ... } )


db.products.find( { "sku": ... , "type": ... } )
db.products.find( { "sku": ... , "type": ... , "name": ... } )

```

## Examples of queries that would be scatter-gather with the compound shard key:

```bash
db.products.find( { "type": ... } )
db.products.find( { "name": ... } )
```

## Targeted Queries vs Scatter Gather: Part 2

## Show collections in the m103 database:

```bash
use m103
show collections

```

## Targeted query with explain() output:

```bash
db.products.find({"sku" : 1000000749 }).explain()

```

## Scatter gather query with explain() output:

```bash
db.products.find( {
  "name" : "Gods And Heroes: Rome Rising - Windows [Digital Download]" }
).explain()

```
